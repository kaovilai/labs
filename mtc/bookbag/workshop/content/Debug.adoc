:sectlinks:
:markup-in-source: verbatim,attributes,quotes
:OCP3_GUID: %ocp3_guid%
:OCP3_DOMAIN: %ocp3_domain%
:OCP3_SSH_USER: %ocp3_ssh_user%
:OCP3_PASSWORD: %ocp3_password%
:OCP4_GUID: %ocp4_guid%
:OCP4_DOMAIN: %ocp4_domain%
:OCP4_SSH_USER: %ocp4_ssh_user%
:OCP4_PASSWORD: %ocp4_password%
:OCP3_BASTION: %ocp3_bastion%
:OCP4_BASTION: %ocp4_bastion%

== Debugging Failed Migrations

Most of the time migrations go as planned, but knowing what to do when they don’t is critical. This lab attempts to provide you with a process/guide to follow when investigating a failed migration.

image:./screenshots/lab7/mig-plan-failed.png[Failed Migration]

=== CRD Architecture

image:./screenshots/lab7/mig-custom-resources.png[Failed Migration]

It’s important to first understand the custom resources in play. The above architecture diagram illustrates the migration resources and their relationships. Most of our debugging failed migration executions will focus on the right side of the diagram: * MigMigration * Velero - Backup and Restore

=== View Debug Resources from WebUI

After a migration event, a link will appear in the WebUI to make it easier to see the specific MTC and Velero resources related to a given MigMigration. 

image:./screenshots/lab7/mig-view-debug-resources_1.png[View migration plan resources (DEBUG) ]


You can view the resources of a migration plan in the MTC web console:

 . Click the link below a migration plan and select **View debug page for more info**.
 .. A new tab will be opened and the migration related resources will be displayed

image:./screenshots/lab7/mig-view-debug-resources_2.png[View migration plan resources (DEBUG) ]

 .. This tree will consist of:
  * Plan - The specific plan being migrated
  ** Migration - The specific MigMigration clicked from the 'Options' or kebab menu
  *** Velero Backup(s)
  **** Velero PodVolumeBackup(s)
  *** Velero Restore(s)
  **** Velero PodVolumeRestore(s)
 .. When looking at a specific resource 2 actions are supported
 ... View the JSON from the resource, by clicking 'View JSON'
 ... Copy the 'oc get' command to the clipboard to aid interaction from CLI

=== Workflow
==== Creation of a MigMigration
Upon execution of a `MigPlan` (stage or migrate), a `MigMigration` is created. This custom resource is created for each distinct run; and is created on the same cluster where the migration-controller that is orchestrating the migration is running.

Each `MigMigration` is named by the controller with a unique UID, if you have a busy cluster that has run several migrations you may find it easier to query for the `MigMigrations` related to a specific plan by leveraging a label *"migration.openshift.io/migplan-name=socks-shop-mig-plan"* applied to each `MigMigration`. 

Let’s take a look at a specific `MigMigration`. On our OCP 4 cluster, perform the following replacing the name of the `MigPlan`

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ oc project openshift-migration

$ oc get migmigration -l 'migration.openshift.io/migplan-name=socks-shop-mig-plan'
NAME                                  READY  PLAN  STAGE  ITINERARY  PHASE
88435fe0-c9f8-11e9-85e6-5d593ce65e10         test  false  Final      Completed
--------------------------------------------------------------------------------

Once you found the appropriate migration execution that you want to investigate, you can request more detail:

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ **oc describe migmigration 88435fe0-c9f8-11e9-85e6-5d593ce65e10 -n openshift-migration**
Name:         88435fe0-c9f8-11e9-85e6-5d593ce65e10
Namespace:    mig
Labels:       <none>
Annotations:  touch: 3b48b543-b53e-4e44-9d34-33563f0f8147
API Version:  migration.openshift.io/v1alpha1
Kind:         MigMigration
Metadata:
  Creation Timestamp:  2019-08-29T01:01:29Z
  Generation:          20
  Resource Version:    88179
  Self Link:           /apis/migration.openshift.io/v1alpha1/namespaces/mig/migmigrations/88435fe0-c9f8-11e9-85e6-5d593ce65e10
  UID:                 8886de4c-c9f8-11e9-95ad-0205fe66cbb6
Spec:
  Mig Plan Ref:
    Name:        socks-shop-mig-plan
    Namespace:   mig
  Quiesce Pods:  true
  Stage:         false
Status:
  Conditions:
    Category:              Advisory
    Durable:               true
    Last Transition Time:  2019-08-29T01:03:40Z
    Message:               The migration has completed successfully.
    Reason:                Completed
    Status:                True
    Type:                  Succeeded
  Phase:                   Completed
  Start Timestamp:         2019-08-29T01:01:29Z
Events:                    <none>
--------------------------------------------------------------------------------

This `MigMigration` describes a successful execution of the socks-shop-mig-plan.

==== Orchestrated Backup and Restore actions
The mig-controller will orchestrate actions on both the source and target clusters. 

Note that the type of migration performed such as a 'Stage' or a 'Migrate' will determine how many Backup and Restore objects will be created.

 *  *Stage* migrations have one Backup and one Restore object, which are focused on _only_ processing the internal Images and Persistent Volumes that will be copied via a filesystem copy.

 * *Final* migrations have two Backup and two Restore objects.
 ** The first Backup object captures the original, unaltered state of the application and its Kubernetes objects.
 ** Then, the application is optionally queisced and a second Backup captures the storage-related resources (PVs, PVCs, and data)

===== Examining a Final migration
====== Source Cluster

Two Velero Backup CRs are created:

`Backup #1`:

[arabic]
. Do an initial backup of k8s resources via Velero (no PV data).
. Annotate all pods with PVs to track what we want to backup.

`Backup #2`:

[arabic]
. If quiesce is selected, scale app down to zero:

* Scales down to zero, Deployment, DeploymentConfig, Job, Statefulset, etc…..all but pods. +
* Standalone pods are left alone, hope is there are minimal of these and most people will use Deployment/ReplicaSets so we can scale to zero. +
* If they had a standalone pod the user is responsible for manual quiesce as they need.

[arabic, start=2]
. Launch `stage' pods, these are used for both stage and migrate, they are a dummy/sleeper pod that just sleeps and mounts the data so we can backup.
. Do a backup of `PV' data via Velero.

*_Note: Velero will sync these Backup CRs between source and target clusters, so they will appear on both clusters._*

====== Target Cluster

Two Velero Restore CRs are created:

`Restore #1`:

[arabic]
. (Uses Backup #2) – Restore just the PV data to destination cluster.

* Do a restore of `PV data', this would be a restore of `Backup #2' above

`Restore #2`:

[arabic]
. (Uses Backup #1) – Restore the k8s resources to the destination cluster.

=== Examining Velero Custom Resources

Let’s take a look at these Velero CRs on our OCP 4 Cluster:

==== Backup

The Velero CRs will contain references to the associated `MigMigration`. We can use the UID of the `MigMigration`, under Metadata, to query the relevant objects:

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ **oc get backup -n openshift-migration -l migmigration=8886de4c-c9f8-11e9-95ad-0205fe66cbb6**
NAME                                         AGE
88435fe0-c9f8-11e9-85e6-5d593ce65e10-59gb7   36m  //Backup 2
88435fe0-c9f8-11e9-85e6-5d593ce65e10-vdjb7   37m  //Backup 1
--------------------------------------------------------------------------------

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ **oc get backup 88435fe0-c9f8-11e9-85e6-5d593ce65e10-59gb7  -n openshift-migration -o yaml**
apiVersion: velero.io/v1
kind: Backup
metadata:
  annotations:
    openshift.io/migrate-copy-phase: final
    openshift.io/migrate-quiesce-pods: "true"
    openshift.io/migration-registry: 172.30.105.179:5000
    openshift.io/migration-registry-dir: /socks-shop-mig-plan-registry-44dd3bd5-c9f8-11e9-95ad-0205fe66cbb6
  creationTimestamp: "2019-08-29T01:03:15Z"
  generateName: 88435fe0-c9f8-11e9-85e6-5d593ce65e10-
  generation: 1
  labels:
    app.kubernetes.io/part-of: migration
    migmigration: 8886de4c-c9f8-11e9-95ad-0205fe66cbb6
    migration-stage-backup: 8886de4c-c9f8-11e9-95ad-0205fe66cbb6
    velero.io/storage-location: myrepo-vpzq9
  name: 88435fe0-c9f8-11e9-85e6-5d593ce65e10-59gb7
  namespace: mig
  resourceVersion: "87313"
  selfLink: /apis/velero.io/v1/namespaces/mig/backups/88435fe0-c9f8-11e9-85e6-5d593ce65e10-59gb7
  uid: c80dbbc0-c9f8-11e9-95ad-0205fe66cbb6
spec:
  excludedNamespaces: []
  excludedResources: []
  hooks:
    resources: []
  includeClusterResources: null
  includedNamespaces:
  - sock-shop
  includedResources:
  - persistentvolumes
  - persistentvolumeclaims
  - namespaces
  - imagestreams
  - imagestreamtags
  - secrets
  - configmaps
  - pods
  labelSelector:
    matchLabels:
      migration-included-stage-backup: 8886de4c-c9f8-11e9-95ad-0205fe66cbb6
  storageLocation: myrepo-vpzq9
  ttl: 720h0m0s
  volumeSnapshotLocations:
  - myrepo-wv6fx
status:
  completionTimestamp: "2019-08-29T01:02:36Z"
  errors: 0
  expiration: "2019-09-28T01:02:35Z"
  phase: Completed
  startTimestamp: "2019-08-29T01:02:35Z"
  validationErrors: null
  version: 1
  volumeSnapshotsAttempted: 0
  volumeSnapshotsCompleted: 0
  warnings: 0
--------------------------------------------------------------------------------

==== Restore

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ **oc get restore -n openshift-migration -l migmigration=8886de4c-c9f8-11e9-95ad-0205fe66cbb6**
NAME                                         AGE
e13a1b60-c927-11e9-9555-d129df7f3b96-gb8nx   15m //Restore 2
e13a1b60-c927-11e9-9555-d129df7f3b96-qnqdt   15m //Restore 1
--------------------------------------------------------------------------------

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ **oc get restore e13a1b60-c927-11e9-9555-d129df7f3b96-gb8nx  -n openshift-migration -o yaml**
apiVersion: velero.io/v1
kind: Restore
metadata:
  annotations:
    openshift.io/migrate-copy-phase: final
    openshift.io/migrate-quiesce-pods: "true"
    openshift.io/migration-registry: 172.30.90.187:5000
    openshift.io/migration-registry-dir: /socks-shop-mig-plan-registry-36f54ca7-c925-11e9-825a-06fa9fb68c88
  creationTimestamp: "2019-08-28T00:09:49Z"
  generateName: e13a1b60-c927-11e9-9555-d129df7f3b96-
  generation: 3
  labels:
    app.kubernetes.io/part-of: migration
    migmigration: e18252c9-c927-11e9-825a-06fa9fb68c88
    migration-final-restore: e18252c9-c927-11e9-825a-06fa9fb68c88
  name: e13a1b60-c927-11e9-9555-d129df7f3b96-gb8nx
  namespace: mig
  resourceVersion: "82329"
  selfLink: /apis/velero.io/v1/namespaces/mig/restores/e13a1b60-c927-11e9-9555-d129df7f3b96-gb8nx
  uid: 26983ec0-c928-11e9-825a-06fa9fb68c88
spec:
  backupName: e13a1b60-c927-11e9-9555-d129df7f3b96-sz24f
  excludedNamespaces: null
  excludedResources:
  - nodes
  - events
  - events.events.k8s.io
  - backups.velero.io
  - restores.velero.io
  - resticrepositories.velero.io
  includedNamespaces: null
  includedResources: null
  namespaceMapping: null
  restorePVs: true
status:
  errors: 0
  failureReason: ""
  phase: Completed
  validationErrors: null
  warnings: 15
--------------------------------------------------------------------------------

=== Controller Logs

Another area we can examine to assist in debugging migration issues is the controller logs.

==== Migration Controller Logs

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ **oc get pods -n openshift-migration | grep controller**
controller-manager-78c469849c-v6wcf           1/1     Running     0          4h49m
--------------------------------------------------------------------------------

==== Combined Migration Logs

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ **oc logs -f -l logreader=mig -n openshift-migration -c color**
controller-manager-78c469849c-v6wcf           1/1     Running     0          4h49m
--------------------------------------------------------------------------------

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ **oc logs controller-manager-78c469849c-v6wcf -f -n mig**
--------------------------------------------------------------------------------

==== Velero Controller Logs

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ **oc get pods -n openshift-migration | grep velero**
velero-7659c69dd7-ctb5x                       1/1     Running     0          4h46m
--------------------------------------------------------------------------------

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ **oc logs velero-7659c69dd7-ctb5x -f -n mig**
--------------------------------------------------------------------------------

==== Restic Controller Logs

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ **oc get pods -n openshift-migration | grep restic**
restic-t4f9b                                  1/1     Running     0          4h47m
--------------------------------------------------------------------------------

[source,subs="{markup-in-source}"]
--------------------------------------------------------------------------------
$ **oc logs restic-t4f9b -f -n openshift-migration**
--------------------------------------------------------------------------------

== Optional Break-fix Exercises

Next, we have created a set of optional break-fix exercises that navigate users through different failure scenarios in MTC that we have commonly observed in an effort to get some hands-on experience and guidance in addressing these conditions.
